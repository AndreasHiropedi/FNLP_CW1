lm_stats=[5579336, 0.3116466822608252, 0.9853501906482038, 1.4536908944718608e-06, 0.005158178014182952, 0.348077988641086]
dev_tweets_preds=[False, False, False, False, False, True, True, False, False, False, False, False, True, False, False, False, True, False, False, True, False, True, True, True, True, False]
answer_short_1_3="Padding can be used to denote the beginning and end of a string or sentence.  \nPadding is a good idea because it helps the model accurately estimate the \nprobabilities of characters or words at the beginning and end of \nstrings or sentences. Consider the string 'za'; since the letter z \nis rarely found at the beginning of English words, padding would result \nin a higher average letter entropy."
answer_short_1_4="p(b|('<s>',)) = [2-gram] 0.046511       bigram probability of the first letter being b\np(b|('b',)) = [2-gram] 0.007750         bigram probability of the next letter being b given the first letter was b\nbacking off for ('b', 'q')              since there was no bigram probability for the next letter being q given the previous letter being b, back off to unigram model\np(q|()) = [1-gram] 0.000892             unigram probability of the letter q\np(q|('b',)) = [2-gram] 0.000092         scaled probability of the unigram model \np(</s>|('q',)) = [2-gram] 0.010636      bigram probability of the last letter being q\n7.85102054894183                        approximate cross-entropy of the n-gram model for the word bbq"
answer_short_1_5='The log histogram makes visualizing the tweet entropies distribution clearer. \nWe see that most tweets have quite a low entropy (within one standard deviation \nfrom the mean), and there are very few points with high entropy. We can use entropy to \ndistinguish between different types of tweets, such as English versus non-English, \nwhere we can assume that non-English tweets will have a higher entropy since our model \nwas trained on the Brown corpus.'
top10_ents=[(2.4921691054394848, ['and', 'here', 'is', 'proof', 'the']), (2.5390025889056127, ['and', 'bailed', 'he', 'here', 'is', 'man', 'on', 'that', 'the']), (2.5584079236733106, ['is', 'the', 'this', 'weather', 'worst']), (2.568653427817313, ['s', 's', 's', 's', 's', 's', 's', 's', 's', 's']), (2.569853705187651, ['be', 'bus', 'here', 'the', 'to', 'want']), (2.576919752608039, ['hell', 'that', 'the', 'was', 'wat']), (2.587767243678531, ['creation', 'is', 'of', 'on', 'story', 'the', 'the']), (2.5885860368906832, ['fro', 'one', 'the', 'the', 'with']), (2.595298329492654, ['is', 'money', 'motive', 'the', 'the']), (2.617870705175611, ['at', 'bucks', 'end', 'lead', 'of', 'the', 'the', 'the'])]
bottom10_ents=[(17.523736748003564, ['作品によっては怪人でありながらヒーロー', 'あるいはその逆', 'というシチュエーションも多々ありますが', 'そうした事がやれるのもやはり怪人とヒーローと言うカテゴリが完成しているからだと思うんですよね', 'あれだけのバリエーションがありながららしさを失わないデザインにはまさに感服です']), (17.524868750262904, ['ロンブーの淳さんはスピリチュアルスポット', 'セドナーで瞑想を実践してた', 'これらは偶然ではなく必然的に起こっている', '自然は全て絶好のタイミングで教えてくれている', 'そして今が今年最大の大改革時期だ']), (17.5264931699585, ['実物経済と金融との乖離を際限なく広げる', 'レバレッジが金融で儲けるコツだと', 'まるで正義のように叫ぶ連中が多いけど', 'これほど不健全な金融常識はないと思う', '連中は不健全と知りながら', '他の奴がやるから出し抜かれる前に出し抜くのが道理と言わんばかりに群がる']), (17.527615646393077, ['一応ワンセット揃えてみたんだけど', 'イマイチ効果を感じないのよね', 'それよりはオーラソーマとか', '肉体に直接働きかけるタイプのアプローチの方が効き目を感じ取りやすい', '波動系ならバッチよりはホメオパシーの方がわかりやすい']), (17.53293217459052, ['慶喜ほどの人でさえこうなんだから', '並の人間だったらなおさら参謀無しじゃ何も出来ない', '一般に吹聴されてる慶喜のネガティブ論は', 'こうした敵対勢力による相次ぐテロに対して終始無関心で', '慶喜個人だけに批判を向けがち']), (17.541019489814225, ['昨日のセミナーではお目にかかれて光栄でした', '楽しく充実した時間をありがとうございました', '親しみのもてる分かりやすい講演に勇気を頂きました', '素晴らしいお仕事とともに益々のご活躍願っております', '今後ともよろしくお願いします']), (17.541411086467402, ['自民党が小沢やめろというなら', '当然町村やめろというブーメランがかえってくるわけです', 'おふたりとも選挙で選ばれた正当な国民の代表ですから', 'できればどちらにもやめてほしくありません', 'そろそろこんな不毛なことはやめにしてほしい']), (17.5427257173663, ['知識欲というのは不随意筋でできている', 'どうせ人間には永久に解明できないんだから', '宇宙はある時点で生まれたのか', 'それとも永遠の過去から存在しているのかなんてことを追究するなと言ってもムダだ', '心臓に止まれと命令しても止まらないのと同じことだ']), (17.547644050965395, ['と言いつつもやっぱり笑えない時はあるよなあ', '笑っても自分の笑顔が汚らわしく思えてすぐ止めちゃうの', '自分が息してるだけで悲しくてぼろぼろ泣いてる時期もあった', '今の自分に必要な経験だったとは思うけど', '出来ればあんな感情は二度とごめんだ']), (17.55280652132174, ['中身の羽毛は精製過程で殺菌処理しているから', '羽毛布団からダニが湧くことはない', 'あと羽毛布団の生地は糸の打ち込み本数が多く', '羽毛の吹き出しを防ぐ目つぶし加工をしているからダニは羽毛ふとんの生地を通過できない', 'ただダニが布団に付着することはあるから手入れは必要'])]
answer_essay_question="        1. Determining the per-word cross-entropy of English poses a great challenge that requires\n        more information in order to obtain a meaningful answer. Additionally, there are several\n        assumptions that this question makes that could be problematic:\n\n`           - One big issue that is not accounted for is that of new words being added to the English\n            language. This would result in sparse data, as the new words would generate zero probability \n            sequences that are gramatical but do not appear in the corpus.\n\n            - Another issue is the omission of the corpus to be used. We know that per-word entropy depends\n            on the frequency of words, which is highly dependent on the context in which different words appear. \n            Therefore, using different corpora from different genres, demographics, time periods, etc., \n            would most likely result in different entropy values. \n\n            - Yet another issue is the ambiguity around the meaning of the term 'English'. This term would need to \n            be clearely defined, as there is no corpus currently that can capture the entirety of the English language;\n            therefore, to produce a more accurate estimate for per-word entropy, a specific subset of the English \n            language should be clearly specified.\n\n        2. One possible experiment to obtain a better estimate for the per-word entropy would involve using a large, diverse \n        corpus, such as the Brown corpus. Before performing any analysis, the data should be cleaned and pre-processed\n        to remove any unwanted characters. The words in the corpus should also be tokenized and converted to lowercase, \n        and we could use word counts to estimate the probabilities of each word. Additionally, we would use our model's cross\n        entropy value to approximate the value for entropy.\n\n        3. The evolution of language over time is influenced by several factors, such as cultural shifts and social\n        changes. As language evolves, new words are introduced to describe new concepts, older words may change meaning or\n        their usage may decrease over time, and the language may undergo lexical diversification. All these changes impact per-word \n        entropy, since per-word entropy measures the uncertainty of a word's occurrence in a given context. Therefore, the use of up-to-date,\n        diversified corpora, as mentioned in the experiment above, would help combat this issue, by providing a greater scope of which words\n        are used more frequently at that time period. Additionally, the use of cross-entropy provides a good approximation for the \n        per-word entropy, as it acts as an upper bound for the true entropy value. \n        "
naive_bayes_vocab_size=13521
naive_bayes_prior={'V': 0.47766934282005674, 'N': 0.5223306571799433}
naive_bayes_likelihood=[0.006913064743369809, 0.0012190937826217086, 0.12333945519178972, 2.2315401420598457e-06, 2.6766530157362866e-05, 0.004917741586184577, 0.004933935254094318]
naive_bayes_posterior=[{'V': 0.5886037204341108, 'N': 0.41139627956588926}, {'V': 0.15633267093794534, 'N': 0.8436673290620547}, {'V': 0.8124219037837241, 'N': 0.18757809621627583}, {'V': 0.8124219037837241, 'N': 0.18757809621627583}, {'V': 0.9961437486715612, 'N': 0.003856251328438816}]
naive_bayes_classify=['V', 'N']
naive_bayes_acc=0.7949987620698192
answer_open_question_2_2="1. The differwence in accuracy implies that feature selection impacts \nthe performance of the logistic regression model. We see that, despite\nall four words providing some degree of information, ultimately a \ncombination of more features results in the highest accuracy (81.08).\nHowever, of all individual features, we see that P provides the most \ninformation (74.13), suggesting that prepositional attachement heavily\nrelies on the choice of P.\n\n2. The observed accuracy in Q2.1 was 79.50%, slightly below that of the \nlogistic regression model. This could be because Naive Bayes assumes \nthat all features are conditionally independent; however, since feature \nP provides more information, its importance should be elevated (which is \naccounted for in the logistic regression model).\n\n3. Using such a feature would reduce the model's ability to generalize to\nunseen data. Additionally, as it provides no information about individual\nfeatures, it would be more beneficial to use individual features."
lr_predictions='VVVVVVVVVNVVVVVVVVVVVVVVVVVVVNVVVVVVVVNVNVVVNVNNNNVVNVVNNNVNNNVNNVVVNNVVVNVNVVNVNVNNNNNVVVNVNNNVNVNVNNVNVVVVNNNNVNNNNVVVNVVNVNNVVNVNNVNVNNNVVNNNNVNNNNVNNNNVVNNNNVNNNNNNNNNNVNNNVVVVVVVNVNVNNNNVVVVNVVVVVVNNVNNVVNVNVNNVNVNVVVNVNVNNNNVNVNNNNVNNVNNVNNVNVVNNNNVNVNVNNVNVVNNNNVVNVNVNVVNVVVNVVNVNNVNNVVNNNVNNVVNNNNNVNVNNVVVNNNVVVVNVVVVVVVNNNNVVVNNNNNVNVNVNNVVNVNNNNNVNVNVVNVVVNNNVVVNVVVVVVNNNNNNVVNVVNNNNNNVNNVVVNNVVVVVVVVNNNVNVNVVNNNVVVVVVVVVNVVVVNVNVNNVVNVVVNVNNNNNVVNVNNNVNVNNNVVNVNNVNNNVNVVVNVNVVVVVVNVVNVNNVNNVNNVVVNVNNVVVVVVVVVVVVVVVVVNVVVNVNNVVNVVNNNNVNVNNNVNVNVNVVNNNNNNVVNNNNVVVVVNNVNVNNNNVNVVNVNVVNVNVVNVVVVNVNNNVVNNVVVNNNNVNVVVVVNVNNVVNVVVVVVVVNVNVVVVNNVNVVNVVNNVNNNNVVVNVNNNVVNNVVVVVVNVVVVVVVVVVVVVVVVNVNVVNNVVVVVVNVNNNNNVVNNVNVNNNNVVNVNVVVNNVNVVNVVVNVNNNNNNNVNNVNVNNVVVVNNNVNNVVVNVVVVVVVVNVNVVVVVNVNVVNVNNNVVNVNNNVNVNVNVVNVVNVVNNNNVNNVVVVVVNVVVVNVNVVVVNNNNVNNNVNVNVNNNNVVNNVNNVVVVVNVNNNNVNNNNVNVVVNNNNNVVNVVNNNVNNVNNNNNNVNNNVNVVVNVVVNVVNNVNNNVVNVVVVNVNVNNNVVVVNNVNNVNNVNNNNVVVVVVVNVVVVVNNVVVVVNNVVVNNNVVVNVVNNVNNNNNNNVVVNNNNVVVVVNVNVNVVVNNVNNVNVVNNNVNVNVNNVNNNVNVVVVVNVVNVNNVVVNNNNNVNNNNVNVNNVNVNNNNVVNNVVNNNNNNVVVVVNVNNVVNVVNNVVVVVVVVVNVVVNVNNVNVVVVNNVVNVVNVVVNVVVVVNNNNNNNNNNNNNNNNNVNNNVNVVNNVVVVNVVNVNVNVVVNVVNNNVNNNVVVNNNNVVNNNVNNNVNNNNVVVNVVNVNNNVVVNVNNVVNVVVNVNVNNNNVVNNNNNVVNNVNNNNNNVVVVNNVNVNVNVNNVVVNVNNVVVNVVNVVNVNNVNNNVNVVVNVVVNVVVVVVVVNVVNNNNVNVVNNNVVNNVNNVNVNNVVVNVNNNVVVNNVVVVNNNNVNVNNVVVVVNNNNNNVVVVNVNVVVVVNVNNNNVVNVVVNNVNVVNNVNNNNNVNNVNNNVNNNNNNVNVNNNNVVVNVVNNVNNVVVNNNNNVNVVVNNNVVVNNVNNNVVVNNNNVVNVNVNNNVNNVNNVNNVNNVNVVNNVNNNNVVNVNVVNVVNNNVNVNNVVVVVVNNVNNVNNVVVVNNNVVVVNVNVNVNVNNVNNNNNNVNVNVNNVVNNVNNNVNVVVVNVNNNVNVNVNNVVNVVNNVNVVNNVVVNNNNVVNNVVNNNVNVNNNVVNVNNVNVNVVNNVNVVNVVNVVVNNNVNNNVNNNNNNNNNNNNNVVVNVVVNVVVVVVNVVNVNVVVNNVNNNNNVVNVNVNVNVNVNVVVNVNNNNVVNVNNVNNVNNNNNNNVNVNVNNVNVNVVVNVVVNNNNNVNVNNNVNVNNVVNNNNNVNNNNNVNNVNVVNNNVVNNNVVVNVNVVVVNNVNVNVVVVVNNNVNNNNVVNVNNNNVNNVNNNNNNVNVNVVVVNVVNNNNVNNVNVVNNNNNVVNNVNNVNNVVNNVVNNVNNNNVNNVNVNVVVVNVNVNVNVVVVNVVVVVNVNVNVNVNNNVVVVNVNVNNVVNNNVVNNVNNVNNNNVVNNVNVNNNNNNVNVVVNNNVVVNNNVVVVVNVVVNNVNVNNVVNNNVVNVVNNVNVVVNVVVNVNVVNNVVVNNVVNNVVVVNNNNNVNVVNNNNNNVNVVVNVVNVNVVNNNVNNVNVNVVNNVVVNNVVVVVVNVNVVNNVNNVVVNVNVNVNVNNVVVVVVVVNNVVNVNVNVNVNNNVNNVVVNNNNVNNVVVNVVNVVNVVNNVNVNNVNVVVNNNVNVVVNVNNVVVVNNNNVVVVVNVVNVVVNVNVVVNVVNNVVVNNNVNNNNVVNNNVVVVNNVVVNNVNVNVVNVVNVNVNNVVVVNNVNVVVVVVVNVNNVVNVNNNNNVVNNVNNNVNNNVNVNVVVNVVVVNVVNNNNNVVVNNNVNVNNVVNVVNNVNVNNNNNVVVVNNVNVNNVNNVVVVVNVNVVVVVVVNNNVNNNVNNVVVVNVNVNVNNVVVNNNVNNNNVNNVVNNVNVNNNNVVVNVVNVNVNNVVNVNVVNVNVVVVVNVNVVVNNVVVVNNNVVVVNNVVVNNNVVVNNVVVNNNNNVVVNVNVVVNNVNNVVNVVVNNNNVNVVVVVVVVVNNVNVVNNNNVNVVNNVVVNNNNVVVNVNNVNNNNNVVNNVVNNVNVNNVVVVVVNVNVVVVVVVVNVNNNNNVVNVVVNVVNNVVNVNNVNVNNNNNNVVNNNVVVVVNNNNNVVVNNVVVVVVNVNNVNNVVNNNNVVNVNNVNNNVNNVVVVVNVVVNNVVVVVVNNVNNNVVVNVVVVVVVVNNVNVNVVNNVVVVVVVVNNVVVNVVVNVVVVNVVVNNVNVNVVVVNVVVVNNNVVVNVVVVNNVVNNVNNVNVNVNNVNVNNNNNVNVNVVVVNNNNNNNNNVNVVNVNVNNNVNVNVNNNNNVVVVVNVNNNVVVNVNVVVVVNNNNNNNVVVVVVNVVVVVNVVVVVNNVVNNVVNVVVNNNNNNNVVNVNNVVVNNNNVVNNVNNVVVVVVVNVVVNVNVVVNVVVVNVNVNNNVNVVNNNNVVVVNVVVNNVNNVNNNVNVNNNNNNNVVVVNVNVNVVVVVVVVVVVVVVVVVNNNVNNVNNVVVVVNVVVNNVVVVNNNNVVVVVVNNNNNVNVNNNVNNNVVNVNVVVVVNNVNNNVVNNVNNNNNVVVVVVVNVNVVVVNVVNNVNVNNNNNVNVVVVNVNNNVVVNNVVNNNNNNVVVNVNVVNNNVVNNNVVVVNVVNVVNNNVVNVVNVNVNNVVNNNVNNVNNVVVVNNVNVNNNVNVNVNNVVVVNVNVNNVNNVNNVVVVNNVNNVVVVVVNVNVVVVVVNVNVVVVNNVVVNNVVNNNNVVNNVVNVVNNNVVVNVNNNNVNNNVVVVVVVVVVNVVNNNNNVVVVNVVNNNVNVNNNNNNNVVNVNNNNVVVNVVNNNNVVVNVVNNNVNNVVVVNNVNNVNVVNVNNNNNVNVVNNNNVVVNNNVVNVVNNNNVVVVVVVNVVNNVVNNNNVVVVVVNNVVVVNNVNVVVVVVVVNVVNVVNNNNNVNVVNVNVVVNVNNNVVNVVVNVVVNNVNVNNNNNNVNVVNVVNVVVVVVNNNNNVVNNNNVVVVVVVNNVNNNNNNVVVNNNVVNNVVNVVNVVVNNVNNVVVVVNVVNNVNNVVVVVNVNVVVVNNNNNNVVNNNNVVVNNNNVVNNVNNVNVNVVVVNNNNVNNVNNNVNVNNVVNVVVNNNVVNNVNVVNNNVNVNVNVNVVVVVVVNNNNVVNVNNNVNNNVNNNNNNNVNVVVVNNNVNNNNNNNNVVNVNVVNNVNVNNNVVNNNNNNNVNVNNNVNVNVNVNVNVNNVNNNVNVNVNVVVNNNVNVNNNNNVNNNNNNVNVVVVNNVNVNNNNNVVNNVNNVVNNNVVVVVVNVVNNNVVNVVVNNVVNVNVNVNVVNVVNNVNVVVVVVVNNNNNNNNVNVNNVVNNNNNVNNVNVNNNVVVVNVNVNVNVNVNVVVNNNNNNVVVVNNNNNNNNVNNVVVNVVVNVVVNVVNNVVVVNNVNVNVNVNNVVNVNNVVVNVVNVVNVVNVNNVN'
best10_features=[(-2.4937916766079735, "('p', 'of')==1 and label is 'V'"), (2.4704648998547976, "('v n2', ('seek', 'Inc.'))==1 and label is 'V'"), (-2.439471374916398, "('n1 p n2', (False, 'of', False))==1 and label is 'V'"), (2.076681820543583, "('v n2', ('following', 'the'))==1 and label is 'V'"), (2.032823037960491, "('v n2', ('been', 'losses'))==1 and label is 'V'"), (-1.9850393428552302, "('n1', 'it')==1 and label is 'N'"), (1.9641373849249528, "('v p', ('assume', 'of'))==1 and label is 'V'"), (1.849618174920253, "('v n2', ('sold', 'bills'))==1 and label is 'V'"), (1.8346672439965972, "('v n2', ('following', 'report'))==1 and label is 'V'"), (1.8104139483823087, "('v n2', ('gained', '33'))==1 and label is 'N'")]
answer_open_question_2_3='We can clearly see from q2.2 that using individual words as features \ncan be informative, and that a combination of words can also help improve\nthe accuracy of the model. Therefore, I tried to incorporate these as part \nof my chosen features, and extend the list of features by also considering \nproper nouns and numbers.\n\nExample 1: (\'verb to\')\nA quite common pattern in English that can indicate noun attachement is the\ncombination of verb and "to". An example of this would be with the verb "rose", \ne.g., "rose to fame"; this pattern works well as a feature and is part of the top \n30 features. However, it is worth noting that "to" alone does not indicate attachement,\nbut in conjunction with certain verbs it does.\n\nExample 2: (n1 p n2)\nThis feature is also part of the top 30 features, and produces a negative correlation\nbetween \'not numeric by numeric\' and the PP being attached to the noun. This is to be \nexpected, as there are plenty of examples in English where this occurs, such as \n\'divide by 10\'. \n\nExample 3: (len n1 n2)\nAlthough this feature was not part of the top 30, it appears that low values of len(n1 + n2)\nseem to indicate verb attachement, and having this feature helps increase accuracy. I assume that\na reasonable explanation for this would be when n1 and n2 are both numeric, and hence len(n1 + n2)\nis lower, this increases the likelihood of verb attachement.'
